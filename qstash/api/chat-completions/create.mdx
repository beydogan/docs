---
title: "Create Chat Completion"
description: "Creates a chat completion of one or more messages"
api: "POST https://qstash.upstash.io/llm/v1/chat/completions"
authMethod: "bearer"
---

Creates a chat completion that generates a textual response
for one or more messages using a large language model.

## Request

<ParamField body="model" type="string" required>
  Name of the model.
</ParamField>

<ParamField body="messages" type="Object[]" required>
  One or more chat messages.
  <Expandable defaultOpen="true">
    <ParamField body="role" type="string" required>
      The role of the message author. One of `system`, `assistant`, or `user`.
    </ParamField>
    <ParamField body="content" type="string" required>
      The content of the message.
    </ParamField>
    <ParamField body="name" type="string">
      An optional name for the participant.
      Provides the model information to differentiate between participants of the same role.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="frequency_penalty" type="number">
  Number between `-2.0` and `2.0`. Positive values penalize new tokens based on their existing
  frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</ParamField>

<ParamField body="logit_bias" type="Object">
  Modify the likelihood of specified tokens appearing in the completion.

  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
  to an associated bias value from `-100` to `100`. Mathematically, the bias is added to
  the logits generated by the model prior to sampling. The exact effect will vary
  per model, but values between `-1` and `1` should decrease or increase likelihood
  of selection; values like `-100` or `100` should result in a ban or exclusive
  selection of the relevant token.
</ParamField>

<ParamField body="logprobs" type="boolean">
  Whether to return log probabilities of the output tokens or not. If true, returns
  the log probabilities of each output token returned in the content of message.
</ParamField>

<ParamField body="top_logprobs" type="number">
  An integer between `0` and `20` specifying the number of most likely tokens to return at
  each token position, each with an associated log probability. logprobs must be set
  to true if this parameter is used.
</ParamField>

<ParamField body="max_tokens" type="number">
  The maximum number of tokens that can be generated in the chat completion.
</ParamField>

<ParamField body="n" type="number">
  How many chat completion choices to generate for each input message.

  Note that you will be charged based on the number of generated tokens
  across all of the choices. Keep `n` as `1` to minimize costs.
</ParamField>

<ParamField body="presence_penalty" type="number">
  Number between `-2.0` and `2.0`. Positive values penalize new tokens
  based on whether they appear in the text so far, increasing the
  model's likelihood to talk about new topics.
</ParamField>

<ParamField body="response_format" type="Object">
  An object specifying the format that the model must output.

  Setting to `{ "type": "json_object" }` enables JSON mode,
  which guarantees the message the model generates is valid JSON.

  **Important**: when using JSON mode, you must also instruct the model
  to produce JSON yourself via a system or user message. Without this,
  the model may generate an unending stream of whitespace until the
  generation reaches the token limit, resulting in a long-running and
  seemingly "stuck" request. Also note that the message content may
  be partially cut off if `finish_reason="length"`, which indicates the
  generation exceeded max_tokens or the conversation exceeded the max context length.
  <Expandable defaultOpen="true">
    <ParamField body="type" type="string" required>
      Must be one of `text` or `json_object`.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="seed" type="number">
  This feature is in Beta. If specified, our system will make a best effort to sample
  deterministically, such that repeated requests with the same seed and parameters
  should return the same result. Determinism is not guaranteed, and you should
  refer to the `system_fingerprint` response parameter to monitor changes in the backend.
</ParamField>

<ParamField body="stop" type="string[]">
  Up to 4 sequences where the API will stop generating further tokens.
</ParamField>

<ParamField body="stream" type="boolean">
  If set, partial message deltas will be sent. Tokens will be sent as
  data-only server-sent events as they become available, with the stream
  terminated by a `data: [DONE]` message.
</ParamField>

<ParamField body="temperature" type="number">
  What sampling temperature to use, between `0` and `2`. Higher values
  like `0.8` will make the output more random, while lower values
  like `0.2` will make it more focused and deterministic.

  We generally recommend altering this or `top_p` but not both.
</ParamField>

<ParamField body="top_p" type="number">
  An alternative to sampling with temperature, called nucleus sampling,
  where the model considers the results of the tokens with `top_p`
  probability mass. So `0.1` means only the tokens comprising the top
  `10%`` probability mass are considered.

  We generally recommend altering this or `temperature` but not both.
</ParamField>

## Response

Returned when `stream` is `false` or not set.

<ResponseField name="id" type="string">
  A unique identifier for the chat completion.
</ResponseField>

<ResponseField name="choices" type="Object[]">
  A list of chat completion choices. Can be more than one if `n` is greater than `1`.
  <Expandable defaultOpen="true">
    <ResponseField name="message" type="Object">
      A chat completion message generated by the model.
      <Expandable defaultOpen="true">
        <ResponseField name="role" type="string">
          The role of the author of this message.
        </ResponseField>
        <ResponseField name="content" type="string">
          The contents of the message.
        </ResponseField>
      </Expandable>
    </ResponseField>
    <ResponseField name="finish_reason" type="string">
      The reason the model stopped generating tokens. This will be `stop` if the
      model hit a natural stop point or a provided stop sequence, `length` if
      the maximum number of tokens specified in the request was reached.
    </ResponseField>
    <ResponseField name="stop_reason" type="string">
      The stop string or token id that caused the completion to stop,
      null if the completion finished for some other reason including
      encountering the EOS token
    </ResponseField>
    <ResponseField name="index" type="number">
      The index of the choice in the list of choices.
    </ResponseField>
    <ResponseField name="logprobs" type="Object">
      Log probability information for the choice.
      <Expandable defaultOpen="false">
        <ResponseField name="content" type="Object[]">
          A list of message content tokens with log probability information.
          <Expandable defaultOpen="false">
            <ResponseField name="token" type="string">
              The token.
            </ResponseField>
            <ResponseField name="logprob" type="number">
              The log probability of this token, if it is within the top 20 most likely tokens.
              Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
            </ResponseField>
            <ResponseField name="bytes" type="number[]">
              A list of integers representing the UTF-8 bytes representation of the token.
              Useful in instances where characters are represented by multiple tokens and
              their byte representations must be combined to generate the correct text
              representation. Can be null if there is no bytes representation for the token.
            </ResponseField>
            <ResponseField name="top_logprobs" type="Object[]">
              List of the most likely tokens and their log probability, at this token position.
              In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
              <Expandable defaultOpen="false">
                <ResponseField name="token" type="string">
                  The token.
                </ResponseField>
                <ResponseField name="logprob" type="number">
                  The log probability of this token, if it is within the top 20 most likely tokens.
                  Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
                </ResponseField>
                <ResponseField name="bytes" type="number[]">
                  A list of integers representing the UTF-8 bytes representation of the token.
                  Useful in instances where characters are represented by multiple tokens and
                  their byte representations must be combined to generate the correct text
                  representation. Can be null if there is no bytes representation for the token.
                </ResponseField>
              </Expandable>
            </ResponseField>
          </Expandable>
        </ResponseField>
      </Expandable>
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="created" type="number">
  The Unix timestamp (in seconds) of when the chat completion was created.
</ResponseField>

<ResponseField name="model" type="string">
  The model used for the chat completion.
</ResponseField>

<ResponseField name="system_fingerprint" type="string">
  This fingerprint represents the backend configuration that the model runs with.

  Can be used in conjunction with the `seed` request parameter to understand
  when backend changes have been made that might impact determinism.
</ResponseField>

<ResponseField name="object" type="string">
  The object type, which is always `chat.completion`.
</ResponseField>

<ResponseField name="usage" type="Object">
  Usage statistics for the completion request.
  <Expandable defaultOpen="true">
    <ResponseField name="completion_tokens" type="number">
      Number of tokens in the generated completion.
    </ResponseField>
    <ResponseField name="prompt_tokens" type="number">
      Number of tokens in the prompt.
    </ResponseField>
    <ResponseField name="total_tokens" type="number">
      Total number of tokens used in the request (prompt + completion).
    </ResponseField>
  </Expandable>
</ResponseField>

## Stream Response

Returned when `stream` is `true`.

<ResponseField name="id" type="string">
  A unique identifier for the chat completion. Each chunk has the same ID.
</ResponseField>

<ResponseField name="choices" type="Object[]">
  A list of chat completion choices. Can be more than one if `n` is greater than `1`.
  Can also be empty for the last chunk.
  <Expandable defaultOpen="true">
    <ResponseField name="delta" type="Object">
      A chat completion delta generated by streamed model responses.
      <Expandable defaultOpen="true">
        <ResponseField name="role" type="string">
          The role of the author of this message.
        </ResponseField>
        <ResponseField name="content" type="string">
          The contents of the chunk message.
        </ResponseField>
      </Expandable>
    </ResponseField>
    <ResponseField name="finish_reason" type="string">
      The reason the model stopped generating tokens. This will be `stop` if the
      model hit a natural stop point or a provided stop sequence, `length` if
      the maximum number of tokens specified in the request was reached.
    </ResponseField>
    <ResponseField name="index" type="number">
      The index of the choice in the list of choices.
    </ResponseField>
    <ResponseField name="logprobs" type="Object">
      Log probability information for the choice.
      <Expandable defaultOpen="false">
        <ResponseField name="content" type="Object[]">
          A list of message content tokens with log probability information.
          <Expandable defaultOpen="false">
            <ResponseField name="token" type="string">
              The token.
            </ResponseField>
            <ResponseField name="logprob" type="number">
              The log probability of this token, if it is within the top 20 most likely tokens.
              Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
            </ResponseField>
            <ResponseField name="bytes" type="number[]">
              A list of integers representing the UTF-8 bytes representation of the token.
              Useful in instances where characters are represented by multiple tokens and
              their byte representations must be combined to generate the correct text
              representation. Can be null if there is no bytes representation for the token.
            </ResponseField>
            <ResponseField name="top_logprobs" type="Object[]">
              List of the most likely tokens and their log probability, at this token position.
              In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
              <Expandable defaultOpen="false">
                <ResponseField name="token" type="string">
                  The token.
                </ResponseField>
                <ResponseField name="logprob" type="number">
                  The log probability of this token, if it is within the top 20 most likely tokens.
                  Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
                </ResponseField>
                <ResponseField name="bytes" type="number[]">
                  A list of integers representing the UTF-8 bytes representation of the token.
                  Useful in instances where characters are represented by multiple tokens and
                  their byte representations must be combined to generate the correct text
                  representation. Can be null if there is no bytes representation for the token.
                </ResponseField>
              </Expandable>
            </ResponseField>
          </Expandable>
        </ResponseField>
      </Expandable>
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="created" type="number">
  The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
</ResponseField>

<ResponseField name="model" type="string">
  The model used for the chat completion.
</ResponseField>

<ResponseField name="system_fingerprint" type="string">
  This fingerprint represents the backend configuration that the model runs with.

  Can be used in conjunction with the `seed` request parameter to understand
  when backend changes have been made that might impact determinism.
</ResponseField>

<ResponseField name="object" type="string">
  The object type, which is always `chat.completion.chunk`.
</ResponseField>

<ResponseField name="usage" type="Object">
  it contains a null value except for the last chunk which contains the token usage statistics for the entire request.
  <Expandable defaultOpen="true">
    <ResponseField name="completion_tokens" type="number">
      Number of tokens in the generated completion.
    </ResponseField>
    <ResponseField name="prompt_tokens" type="number">
      Number of tokens in the prompt.
    </ResponseField>
    <ResponseField name="total_tokens" type="number">
      Total number of tokens used in the request (prompt + completion).
    </ResponseField>
  </Expandable>
</ResponseField>

<RequestExample>

```sh curl
curl "https://qstash.upstash.io/llm/v1/chat/completions" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is the capital of Turkey?"
            }
        ]
    }'
```

</RequestExample>

<ResponseExample>
```json 200 OK
{
  "id": "cmpl-abefcf66fae945b384e334e36c7fdc97",
  "object": "chat.completion",
  "created": 1717483987,
  "model": "meta-llama/Meta-Llama-3-8B-Instruct",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The capital of Turkey is Ankara."
      },
      "logprobs": null,
      "finish_reason": "stop",
      "stop_reason": null
    }
  ],
  "usage": {
    "prompt_tokens": 18,
    "total_tokens": 26,
    "completion_tokens": 8
  }
}
```

```json 200 OK - Stream
data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"role":"assistant"},"logprobs":null,"finish_reason":null}]}

data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"content":"The"},"logprobs":null,"finish_reason":null}]}

data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"content":" capital"},"logprobs":null,"finish_reason":null}]}

data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"content":" of"},"logprobs":null,"finish_reason":null}]}

data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"content":" Turkey"},"logprobs":null,"finish_reason":null}]}

data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"content":" is"},"logprobs":null,"finish_reason":null}]}

data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"content":" Ankara"},"logprobs":null,"finish_reason":null}]}

data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"content":"."},"logprobs":null,"finish_reason":null}]}

data: {"id":"cmpl-dfc1ad80d0254c2aaf3e7775d1830c9d","object":"chat.completion.chunk","created":1717484084,"model":"meta-llama/Meta-Llama-3-8B-Instruct","choices":[{"index":0,"delta":{"content":""},"finish_reason":"stop"}],"usage":{"prompt_tokens":18,"total_tokens":26,"completion_tokens":8}}

data: [DONE]
```
</ResponseExample>